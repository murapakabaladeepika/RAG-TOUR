{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL & Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Notes\n",
    "- Expose schematic information about their input, output and config through their `.input_schema` property, the `.output_schema` property and the `.config_schema` method\n",
    "- LCEL is a declarative way to compose Runnables into chains - any chain constructed with LCEL will also always automatically have sync, async, batch, and streaming support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Primitives\n",
    "#### 1. RunnableSequence\n",
    "- Invokes a series of Runnables sequentially\n",
    "- output of the previous runnable will be input to the next runnable in the chain\n",
    "- can use pipe operator to construct or by passing a list of RunnableSequence\n",
    "- \n",
    "#### 2. Runnable Parallel\n",
    "- Invokes runnables concurrently, providing the same input to each. Construct using dict literal within a sequence or by passing a dict to RunnableParallel\n",
    "\n",
    "#### 3. RunnablePassThrough\n",
    "- Used as a \"passthrough\" take it takes any input to the current component  and allows us to provide it in the component output via the\n",
    "  \"Question\" key\n",
    "#### 4. Runnable Lambda \n",
    "- RunnableLambda converts a python callable into a Runnable.\r",
    "- \r\n",
    "Wrapping a callable in a RunnableLambda makes the callable usable within either a sync or async context- \r\n",
    "\r\n",
    "RunnableLambda can be composed as any other Runnable and provides seamless integration with LangChain tracing.\n",
    "\n",
    "### Additional Methods\n",
    "- can be used to modify behavior such as retry policy, lifecycle listeners, make them configurable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "- You can use `set_debug` from `from langchain_core.globals import set_debug`. set_debug(True)\n",
    "- You can pass existing or custom callbacks to any give chain too:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```from langchain_core.tracers import ConsoleCallbackHandler\n",
    "\n",
    "chain.invoke(\n",
    "    ...,\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]}\n",
    ")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Data through\n",
    "- RunnablePassthrough allows to pass inputs unchanged or with the addition of extra keys. This typically is used in conjuction with RunnableParallel to assign data to a new key in the map.\n",
    "- RunnablePassthrough() called on itâ€™s own, will simply take the input and pass it through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `passed` key was called with RunnablePassthrough() and so it passed on {'num': 1}.\n",
    "\n",
    "- In the second line with the key `extra`, we used RunnablePastshrough.assign with a lambda that multiplies the passed integer value by 3. So, `extra` was set with {'num': 1, 'mult': 3} which is the original value with the `'mult'` key added.\n",
    "\n",
    "- Finally, we set a third key called `modified` in the map which uses a labmda to set a single value adding 1 to the num, which resulted in modified key with the value of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "All inputs to these functions need to be a **SINGLE** argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single input and unpacks it into multiple argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use `itemgetter` to extract the input arguments\n",
    "- `\"a\"` becomes `3` because `itemgetter` gets the value of `\"foo\"` and that is passed through pipe operator to the RunnableLamba which passes the value to the `length_function` which returns length of `\"bar\"` which is `3`\n",
    "- `\"b\"` is 9 because we pass the dictionary of `{\"text1\": \"bar\", \"text2\": \"gah\"}` to the RunnableLambda that passes it into the `multiple_length_function`\n",
    "- the dictionary of `{\"a\":3, \"b\": 9}` is then passed as output to the prompt usuing the pipe operator (`|`)\n",
    "- the output of the prompt again is passed to the model\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binding runtime arguments\n",
    "- we can use `Runnable.bind()` to pass arguments as constants so that we can have access to them even within a runnable sequence where the argument is not part of the output of preceding runnables in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[4, 6, 8]\n",
      "<generator object RunnableSequence.stream at 0x0000024106C772E0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# A RunnableSequence constructed using the `|` operator\n",
    "sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n",
    "print(sequence.invoke(1)) \n",
    "print(sequence.batch([1, 2, 3]))\n",
    "print(sequence.stream(2))\n",
    "await sequence.ainvoke(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mul_2': 6, 'mul_5': 15, 'pow_2': 9}, {'mul_2': 8, 'mul_5': 20, 'pow_2': 16}, {'mul_2': 10, 'mul_5': 25, 'pow_2': 25}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'mul_2': 6, 'mul_5': 15, 'pow_2': 9},\n",
       " {'mul_2': 8, 'mul_5': 20, 'pow_2': 16},\n",
       " {'mul_2': 10, 'mul_5': 25, 'pow_2': 25}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = RunnableLambda(lambda x: x + 2) | {\n",
    "    'mul_2': RunnableLambda(lambda x: x * 2),\n",
    "    'mul_5': RunnableLambda(lambda x: x * 5),\n",
    "    \"pow_2\": RunnableLambda(lambda x: x**2)\n",
    "}\n",
    "print(sequence.map().invoke([1,2,3])) \n",
    "sequence.batch([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "model = ChatAnthropic(model_name=\"claude-3-haiku-20240307\",api_key='your_api_key\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a funny joke about friends:\\n\\nWhy can\\'t a bicycle stand up on its own? It\\'s two-tired!\\n\\nThis joke plays on the similar pronunciation of \"too tired\" and \"two-tired\" to create a humorous pun about a bicycle not being able to stand up. The punchline is a play on words that relies on the shared sound between \"too\" and \"two\" to create an unexpected and silly joke.\\n\\nJokes about friends often involve humorous situations or misunderstandings between buddies. The shared experience and camaraderie between friends is a common theme in comedy. I hope you enjoyed this lighthearted joke! Let me know if you\\'d like to hear another one.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'topic':'friend'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "\n",
    "composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a short and silly joke about friends:\\n\\nWhy can\\'t a bicycle stand up on its own? It\\'s two-tired!\\n\\n(Get it? \"Two-tired\" sounds like \"too tired\", implying the bicycle is tired and can\\'t stand up on its own, just like a friend who is too tired to support you!)\\n\\nHumor is very subjective, so not everyone may find this joke funny. But the idea is to play on words in a lighthearted way to poke fun at the relationship between friends. Hopefully you get a chuckle out of this simple friend-related joke!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'topic':'friend'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_chain_with_lambda = (\n",
    "    chain\n",
    "    | (lambda input: {\"joke\": input})\n",
    "    | analysis_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That\\'s a cute pun-based dog joke! I chuckled at the play on the \"two left feet\" idiom. Puns can be a bit hit or miss, but this one lands nicely in the silly and mildly amusing category. Dog jokes can be a lot of fun - I\\'d be happy to hear another one if you have another good pun or silly observation about our canine friends. Comedic delivery is an art, but you pulled this one off well!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain_with_lambda.invoke({'topic':'dogs'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'modified': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'add': 4, 'mul': 8}, {'add': 5, 'mul': 12}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'add': 4, 'mul': 8}, {'add': 5, 'mul': 12}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda, RunnableAssign\n",
    "\n",
    "def add_num(a):\n",
    "    return a+2\n",
    "def mul_num(c):\n",
    "    return c*4\n",
    "\n",
    "chain = RunnableParallel({'add': RunnableLambda(add_num) , 'mul': RunnableLambda(mul_num)})\n",
    "\n",
    "print(chain.map().invoke([2,3]))\n",
    "chain.batch([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 5, 'add_step': {'added': 50}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 5, 'add_step': {'added': 50}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda, RunnableAssign\n",
    "from typing import Dict\n",
    "\n",
    "def mul_ten(x: Dict[str, int]):\n",
    "    return {\"added\": x[\"input\"] * 10}\n",
    "\n",
    "mapper = RunnableParallel(\n",
    "    {\"add_step\": RunnableLambda(mul_ten),}\n",
    ")\n",
    "\n",
    "runnable_assign = RunnableAssign(mapper)\n",
    "\n",
    "# Synchronous example\n",
    "print(runnable_assign.invoke({\"input\": 5}))\n",
    "\n",
    "\n",
    "# Asynchronous example\n",
    "await runnable_assign.ainvoke({\"input\": 5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable Message History\n",
    "- Runnable that manages chat message history for another Runnable.\n",
    "\n",
    "- A chat message history is a sequence of messages that represent a conversation.\n",
    "\n",
    "- RunnableWithMessageHistory wraps another Runnable and manages the chat message history for it; it is responsible for reading and updating the chat message history.\n",
    "\n",
    "- The formats supports for the inputs and outputs of the wrapped Runnable are described below.\n",
    "\n",
    "- RunnableWithMessageHistory must always be called with a config that contains the appropriate parameters for the chat message history factory.\n",
    "\n",
    "- By default the Runnable is expected to take a single configuration parameter called session_id which is a string. This parameter is used to create a new or look up an existing chat message history that matches the given session_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is a trigonometric function that describes the ratio of the adjacent side to the hypotenuse of a right-angled triangle.\\n\\nMore specifically, in a right-angled triangle:\\n\\n- The cosine of an angle is the ratio of the length of the adjacent side to the length of the hypotenuse.\\n\\nThe cosine function is represented by the abbreviation cos or cos(), and is one of the basic trigonometric functions studied in mathematics along with sine and tangent.\\n\\nSome key properties of the cosine function:\\n\\n- It oscillates between -1 and 1\\n- The cosine of 0Â° is 1\\n- The cosine of 90Â° is 0 \\n- The cosine function is periodic with a period of 360Â°\\n- It is commonly used in physics, engineering, navigation and many other fields involving periodic phenomena.\\n\\nSo in summary, cosine expresses the ratio of sides in a right triangle in relation to one of the acute angles. It is an important concept in trigonometry and analytical geometry.', additional_kwargs={'usage': {'prompt_tokens': 22, 'completion_tokens': 230, 'total_tokens': 252}}, response_metadata={'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0', 'usage': {'prompt_tokens': 22, 'completion_tokens': 230, 'total_tokens': 252}}, id='run-8f4459d1-813b-4e3c-9b3f-62a9f426bb8d-0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "from typing import Optional\n",
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"In memory implementation of chat message history.\"\"\"\n",
    "\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]):\n",
    "        \"\"\"Add a list of messages to the store\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "def get_session_history(\n",
    "    user_id: str, conversation_id: str):\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = InMemoryHistory()\n",
    "    return store[(user_id, conversation_id)]\n",
    "    \n",
    "def get_by_session_id(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime', \n",
    "    region_name='us-east-1'\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're an assistant who's good at {ability}\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "model = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",client=bedrock_runtime)\n",
    "\n",
    "chain = prompt | model\n",
    "store= {}\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,                \n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"question\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
